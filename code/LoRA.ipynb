{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 22:07:25.952996: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, roc_curve, auc\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "import pandas as pd\n",
    "import bitsandbytes, accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom batch preprocessor\n",
    "def collate_fn(batch, tokenizer):\n",
    "    dict_keys = ['input_ids', 'attention_mask', 'labels']\n",
    "    d = {k: [dic[k] for dic in batch] for k in dict_keys}\n",
    "    d['input_ids'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['input_ids'], batch_first=True, padding_value=float(pad_token_id)\n",
    "    )\n",
    "    d['attention_mask'] = torch.nn.utils.rnn.pad_sequence(\n",
    "        d['attention_mask'], batch_first=True, padding_value=float(pad_token_id)\n",
    "    )\n",
    "    d['labels'] = torch.stack(d['labels'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom trainer class to be able to pass label weights and calculate mutilabel loss\n",
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, label_weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.label_weights = label_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # compute custom loss\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels.to(torch.float32), pos_weight=self.label_weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "N_LABELS = 30 #主标签+辅助标签\n",
    "\n",
    "# def preprocess(data):\n",
    "#     '''\n",
    "#     Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "#     '''\n",
    "#     punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "#     def clean_special_chars(text, punct):\n",
    "#         for p in punct:\n",
    "#             text = text.replace(p, ' ')\n",
    "#         return text\n",
    "\n",
    "#     data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None\n",
    "\n",
    "train = pd.read_csv('../dataset/train.csv')\n",
    "# test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "# x_train = preprocess(train['comment_text'])\n",
    "x_train = train['comment_text']\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "# y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "y_aux_train = train[['target']+list(train.columns[3:32])]\n",
    "y_aux_train = y_aux_train.applymap(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "\n",
    "# 均衡化数据集\n",
    "# 确定数量较少的类别\n",
    "count_class_0 = np.sum(y_train == 0)\n",
    "count_class_1 = np.sum(y_train == 1)\n",
    "\n",
    "# 从数量较多的类别中随机抽样\n",
    "index_class_0 = np.where(y_train == 0)[0]\n",
    "index_class_1 = np.where(y_train == 1)[0]\n",
    "\n",
    "# 从数量较多的类别中随机选择与数量较少的类别相同数量的样本\n",
    "seed_everything()\n",
    "selected_index_class_0 = np.random.choice(index_class_0, count_class_1, replace=False)\n",
    "\n",
    "# 构建平衡的数据集\n",
    "x_train = np.concatenate([x_train[selected_index_class_0], x_train[index_class_1]])\n",
    "y_train = np.concatenate([y_train[selected_index_class_0], y_train[index_class_1]])\n",
    "y_aux_train = np.concatenate([y_aux_train.iloc[selected_index_class_0], y_aux_train.iloc[index_class_1]])\n",
    "\n",
    "# MAX_LEN = 220\n",
    "# 划分训练集和测试集，按照9:1的比例\n",
    "x_train, x_test, y_train, y_test, y_aux_train, y_aux_test = train_test_split(\n",
    "    x_train, y_train, y_aux_train, test_size=0.1, random_state=1624)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hf dataset\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': x_train, 'labels': y_aux_train}),\n",
    "    'val': Dataset.from_dict({'text': x_test, 'labels': y_aux_test})\n",
    "})\n",
    "label_weights = np.ones((N_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a972acaa04d86877b91c2e4ccd06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/259801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2765f18c43d47c99baae3e604be9185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model name\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# preprocess dataset with tokenizer\n",
    "def tokenize_examples(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['text'])\n",
    "    tokenized_inputs['labels'] = examples['labels']\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./BERT')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\n",
    "tokenized_ds = tokenized_ds.with_format('torch')\n",
    "pad_token_id = 0\n",
    "\n",
    "# # quantization config\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # enable 4-bit quantization\n",
    "#     bnb_4bit_quant_type='nf4',  # information theoretically optimal dtype for normally distributed weights\n",
    "#     bnb_4bit_use_double_quant=True,  # quantize quantized weights //insert xzibit meme\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16  # optimized fp format for ML\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./BERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 96,798 || all params: 109,602,108 || trainable%: 0.0883\n"
     ]
    }
   ],
   "source": [
    "# lora config\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # the dimension of the low-rank matrices\n",
    "    inference_mode=False,\n",
    "    lora_alpha=8,  # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules=[#'query', \n",
    "                    # 'key', \n",
    "                    'value',\n",
    "                    'classifier.bias',\n",
    "                    'classifier.weight'\n",
    "                    'dense'\n",
    "                   ],\n",
    "    lora_dropout=0.1,  # dropout probability of the LoRA layers\n",
    "    bias='none',  # whether to train bias weights, so 'none' for attention layers\n",
    "    task_type='SEQ_CLS'\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    './BERT',\n",
    "    num_labels=N_LABELS\n",
    ")\n",
    "# print(model)\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.config.pad_token_id = pad_token_id\n",
    "\n",
    "# define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'multilabel_classification',\n",
    "    learning_rate = 5e-4,\n",
    "    per_device_train_batch_size = 8, \n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = torch.sigmoid(torch.tensor(logits)).numpy() > 0.5\n",
    "#     # labels = labels.numpy()\n",
    "    \n",
    "#     # # Calculate F1 scores\n",
    "#     # f1_micro = f1_score(labels, predictions, average='micro')\n",
    "#     # f1_macro = f1_score(labels, predictions, average='macro')\n",
    "#     # f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "#     # # Plot Confusion Matrix for each label\n",
    "#     # conf_matrices = multilabel_confusion_matrix(labels, predictions)\n",
    "#     # fig, ax = plt.subplots(1, len(conf_matrices), figsize=(15, 5))\n",
    "#     # # if len(conf_matrices) > 1:\n",
    "#     # #     for idx, cm in enumerate(conf_matrices):\n",
    "#     # #         plot_confusion_matrix(cm, idx, ax[idx])\n",
    "#     # # else:\n",
    "#     # #     plot_confusion_matrix(conf_matrices[0], 0, ax)\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "#     # # Plot ROC Curves\n",
    "#     # plot_multilabel_roc(labels, torch.sigmoid(torch.tensor(logits)).numpy(), num_classes=labels.shape[1])\n",
    "    \n",
    "#     return {}\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "def compute_metrics(pred):\n",
    "    # 获取预测的概率值\n",
    "    logits = pred.predictions\n",
    "    probabilities = 1 / (1 + np.exp(-logits))  # sigmoid 转换\n",
    "\n",
    "    # 获取真实的标签\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # 选择第一个类别的预测和真实标签\n",
    "    class_index = 0  # 第一个类别的索引\n",
    "    predictions_class = (probabilities[:, class_index] > 0.5).astype(int)\n",
    "    labels_class = labels[:, class_index]\n",
    "\n",
    "    # 计算第一个类别的准确度和召回率\n",
    "    accuracy_class = accuracy_score(labels_class, predictions_class)\n",
    "    recall_class = recall_score(labels_class, predictions_class)\n",
    "\n",
    "    return {\n",
    "        'accuracy_class_0': accuracy_class,\n",
    "        'recall_class_0': recall_class,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以反复运行以下代码，起到增加epoch数量的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64952' max='64952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64952/64952 56:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.029688</td>\n",
       "      <td>0.840337</td>\n",
       "      <td>0.897926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.026909</td>\n",
       "      <td>0.854471</td>\n",
       "      <td>0.872455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=64952, training_loss=0.03149752416315888, metrics={'train_runtime': 3375.1759, 'train_samples_per_second': 153.948, 'train_steps_per_second': 19.244, 'total_flos': 4.438001751174108e+16, 'train_loss': 0.03149752416315888, 'epoch': 2.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "trainer = CustomTrainer(\n",
    "    model = model.cuda(),\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_ds['train'],\n",
    "    eval_dataset = tokenized_ds['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = functools.partial(collate_fn, tokenizer=tokenizer),\n",
    "    compute_metrics = compute_metrics,\n",
    "    label_weights = torch.tensor(label_weights, device=model.device)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora-bert-2epoch/tokenizer_config.json',\n",
       " 'lora-bert-2epoch/special_tokens_map.json',\n",
       " 'lora-bert-2epoch/vocab.txt',\n",
       " 'lora-bert-2epoch/added_tokens.json',\n",
       " 'lora-bert-2epoch/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "peft_model_id = 'lora-bert-2epoch'\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64952' max='64952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64952/64952 56:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Class 0</th>\n",
       "      <th>Recall Class 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.854263</td>\n",
       "      <td>0.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>0.857034</td>\n",
       "      <td>0.880788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./BERT - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('lora-bert-4epoch/tokenizer_config.json',\n",
       " 'lora-bert-4epoch/special_tokens_map.json',\n",
       " 'lora-bert-4epoch/vocab.txt',\n",
       " 'lora-bert-4epoch/added_tokens.json',\n",
       " 'lora-bert-4epoch/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#再训练两轮\n",
    "trainer.train()\n",
    "# save model\n",
    "peft_model_id = 'lora-bert-4epoch'\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "在进行模型评测时，以下需要补充读出模型，输入训练集，进行预测的代码\n",
    "\n",
    "这部分就交给你们啦（因为我单模型的precision和recall已经通过上面得到了）\n",
    "\n",
    "我的训练集：标签均为0-1，共N_LABELS个（可以讨论只用toxicity或者identity的情况），模型的输出需要进一步sigmoid才会得到概率。在集成学习中，把得到的label综合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
