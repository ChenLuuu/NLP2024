{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 14:56:28.064048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 14:56:28.103616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 14:56:28.720527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "   return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理函数\n",
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed.\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "df = pd.read_csv('../dataset/train-tagged.csv')\n",
    "\n",
    "# 过滤 identity_annotator_count 大于 0 的行\n",
    "df_filtered = df[df['identity_annotator_count'] > 0]\n",
    "\n",
    "# 提取需要的列\n",
    "texts = df_filtered['comment_text']\n",
    "labels = df_filtered[['target', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']].values\n",
    "labels = labels * 10\n",
    "\n",
    "texts = preprocess(texts)\n",
    "\n",
    "# 使用 Tokenizer 进行 tokenization 和序列化\n",
    "tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/bert-base-uncased')\n",
    "\n",
    "# 将文本转换为token序列\n",
    "sequences = [tokenizer.encode(text, add_special_tokens=True) for text in tqdm(texts, desc=\"Tokenizing\")]\n",
    "\n",
    "print(\"Tokenization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([405130, 220])\n",
      "torch.Size([405130, 25])\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# 设置序列的最大长度\n",
    "maxlen = 220\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# 将数据转换为 Tensor\n",
    "text_tensor = torch.tensor(padded_sequences, dtype=torch.long, device='cuda:0')\n",
    "label_tensor = torch.tensor(labels, dtype=torch.float, device='cuda:0')\n",
    "\n",
    "# 打印 Tensor 的形状以确认\n",
    "print(text_tensor.shape)\n",
    "print(label_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,  101, 2023, 2003, 1037, 2307, 2466, 2158, 1045,\n",
      "        4687, 2065, 1996, 2711, 2040, 7581, 3844, 1996, 6616, 2039, 2012, 2032,\n",
      "        2412, 2657, 2009,  102], device='cuda:0')\n",
      "tensor([4.4000, 6.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(text_tensor[1])\n",
    "print(label_tensor[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TensorDataset(text_tensor, label_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFineTuner(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertFineTuner, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(bert_model.config.hidden_size, 25)  # 输出一个标量\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('/root/autodl-tmp/bert-base-uncased')\n",
    "# 实例化模型\n",
    "model = BertFineTuner(bert_model)\n",
    "model = model.cuda()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 12661/12661 [38:14<00:00,  5.52it/s, loss=0.141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.28955142467922157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 12661/12661 [38:13<00:00,  5.52it/s, loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.2093714432223446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 12661/12661 [38:14<00:00,  5.52it/s, loss=0.0558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.18723126015418534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 12661/12661 [38:14<00:00,  5.52it/s, loss=0.0927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.16501742694725519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 12661/12661 [38:18<00:00,  5.51it/s, loss=0.09]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.14336694270636025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # 假设训练 3 个 epoch\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch_inputs, batch_labels = batch\n",
    "        batch_inputs = batch_inputs.cuda()\n",
    "        batch_labels = batch_labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存微调后的模型\n",
    "torch.save(model.state_dict(), \"/root/autodl-tmp/code/models/finetuned_bert_on_identity.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertFineTuner(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的模型\n",
    "model = BertFineTuner(bert_model)\n",
    "model.load_state_dict(torch.load('/root/autodl-tmp/code/models/finetuned_bert_on_identity.pth'))\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 CSV 文件\n",
    "readdata = pd.read_csv('/root/autodl-tmp/dataset/train-tagged.csv')\n",
    "\n",
    "# 过滤出 identity_annotator_count 列值为 0 的数据\n",
    "data_to_predict = readdata[readdata['identity_annotator_count'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(readdata.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed.\n"
     ]
    }
   ],
   "source": [
    "dt = data_to_predict['comment_text']\n",
    "dt = preprocess(dt)\n",
    "tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/bert-base-uncased')\n",
    "to_predict = [tokenizer.encode(text, add_special_tokens=True) for text in tqdm(dt, desc=\"Tokenizing\")]\n",
    "print(\"Tokenization completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1399744, 220])\n"
     ]
    }
   ],
   "source": [
    "padded_to_predict = pad_sequences(to_predict, maxlen=220)\n",
    "\n",
    "# 将数据转换为 Tensor\n",
    "topredict_tensor = torch.tensor(padded_to_predict, dtype=torch.long, device='cuda:0')\n",
    "\n",
    "# 打印 Tensor 的形状以确认\n",
    "print(topredict_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(topredict_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# 预测函数\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        batch_inputs = batch[0]\n",
    "        outputs = model(batch_inputs)\n",
    "        predictions = outputs.cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "all_predictions = all_predictions / 10 # 为了对应先前的x\n",
    "predictions_df = pd.DataFrame(all_predictions, columns=['label1', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability'])\n",
    "\n",
    "# 丢弃第一个标签，也就是预测的target值\n",
    "predictions_df = predictions_df.drop(columns=['label1'])\n",
    "\n",
    "# 将预测结果写回原数据\n",
    "readdata.loc[readdata['identity_annotator_count'] == 0, ['male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']] = predictions_df.values\n",
    "\n",
    "# 保存更新后的 CSV 文件\n",
    "readdata.to_csv('/root/autodl-tmp/dataset/train-tagged.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = pd.read_csv('/root/autodl-tmp/dataset/train-tagged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将值小于0的替换为0，大于1替换为1\n",
    "columns_to_replace = ['male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n",
    "process_data[columns_to_replace] = process_data[columns_to_replace] \n",
    "process_data[columns_to_replace] = process_data[columns_to_replace].apply(lambda x: x.clip(0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data.to_csv('/root/autodl-tmp/dataset/train-tagged.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id    target                                       comment_text  \\\n",
      "0   59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
      "1   59849  0.000000  Thank you!! This would make my life a lot less...   \n",
      "2   59852  0.000000  This is such an urgent design problem; kudos t...   \n",
      "3   59855  0.000000  Is this something I'll be able to install on m...   \n",
      "4   59856  0.893617               haha you guys are a bunch of losers.   \n",
      "5   59859  0.666667                               ur a sh*tty comment.   \n",
      "6   59861  0.457627                        hahahahahahahahhha suck it.   \n",
      "7   59863  0.000000                                FFFFUUUUUUUUUUUUUUU   \n",
      "8  239575  0.000000  The ranchers seem motivated by mostly by greed...   \n",
      "9  239576  0.000000  It was a great show. Not a combo I'd of expect...   \n",
      "\n",
      "   severe_toxicity   obscene  identity_attack    insult    threat     asian  \\\n",
      "0         0.001123  0.001111         0.000781  0.015544  0.001114  0.001913   \n",
      "1         0.001383  0.001711         0.001993  0.009439  0.006587  0.002326   \n",
      "2         0.002714  0.004068         0.004758  0.006904  0.006082  0.002767   \n",
      "3         0.002567  0.003380         0.003236  0.006296  0.007838  0.002545   \n",
      "4         0.021277  0.000000         0.021277  0.872340  0.000000  0.000000   \n",
      "5         0.047619  0.638095         0.000000  0.333333  0.000000  0.003018   \n",
      "6         0.050847  0.305085         0.000000  0.254237  0.000000  0.009625   \n",
      "7         0.002902  0.021125         0.000000  0.023498  0.001091  0.002390   \n",
      "8         0.001021  0.000613         0.002658  0.016227  0.001222  0.001344   \n",
      "9         0.002045  0.002890         0.002492  0.011073  0.003888  0.000000   \n",
      "\n",
      "    atheist  ...    rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
      "0  0.002266  ...  rejected      0    0    0      0         0         0.000000   \n",
      "1  0.003480  ...  rejected      0    0    0      0         0         0.000000   \n",
      "2  0.000022  ...  rejected      0    0    0      0         0         0.000000   \n",
      "3  0.003344  ...  rejected      0    0    0      0         0         0.000000   \n",
      "4  0.000000  ...  rejected      0    0    0      1         0         0.000000   \n",
      "5  0.001057  ...  rejected      0    0    0      0         0         0.009524   \n",
      "6  0.002271  ...  rejected      0    0    0      0         0         0.220339   \n",
      "7  0.001201  ...  rejected      0    0    0      0         0         0.000000   \n",
      "8  0.000669  ...  approved      0    0    0      0         0         0.000000   \n",
      "9  0.001314  ...  approved      0    0    0      1         0         0.000000   \n",
      "\n",
      "   identity_annotator_count  toxicity_annotator_count  toxicity_sum  \n",
      "0                         0                         4      0.000000  \n",
      "1                         0                         4      0.000000  \n",
      "2                         0                         4      0.000000  \n",
      "3                         0                         4      0.000000  \n",
      "4                         4                        47      0.914894  \n",
      "5                         0                       105      1.019048  \n",
      "6                         0                        59      0.610169  \n",
      "7                         0                         4      0.000000  \n",
      "8                         0                         4      0.000000  \n",
      "9                         0                         4      0.000000  \n",
      "\n",
      "[10 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# 查看一下预测好的值\n",
    "print(process_data.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
